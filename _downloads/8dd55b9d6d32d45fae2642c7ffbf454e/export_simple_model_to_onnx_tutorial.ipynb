{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMbEB3mp3CrC"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7gP-Qnz3CrE"
      },
      "source": [
        "[Introduction to ONNX](intro_onnx.html) \\|\\| **Exporting a PyTorch model\n",
        "to ONNX** \\|\\| [Extending the ONNX\n",
        "Registry](onnx_registry_tutorial.html)\n",
        "\n",
        "Export a PyTorch model to ONNX\n",
        "==============================\n",
        "\n",
        "**Author**: [Thiago Crepaldi](https://github.com/thiagocrepaldi)\n",
        "\n",
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>As of PyTorch 2.1, there are two versions of ONNX Exporter.</p>\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cukly6o3CrG"
      },
      "source": [
        "In the [60 Minute\n",
        "Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html),\n",
        "we had the opportunity to learn about PyTorch at a high level and train\n",
        "a small neural network to classify images. In this tutorial, we are\n",
        "going to expand this to describe how to convert a model defined in\n",
        "PyTorch into the ONNX format using TorchDynamo and the\n",
        "`torch.onnx.dynamo_export` ONNX exporter.\n",
        "\n",
        "While PyTorch is great for iterating on the development of models, the\n",
        "model can be deployed to production using different formats, including\n",
        "[ONNX](https://onnx.ai/) (Open Neural Network Exchange)!\n",
        "\n",
        "ONNX is a flexible open standard format for representing machine\n",
        "learning models which standardized representations of machine learning\n",
        "allow them to be executed across a gamut of hardware platforms and\n",
        "runtime environments from large-scale cloud-based supercomputers to\n",
        "resource-constrained edge devices, such as your web browser and phone.\n",
        "\n",
        "In this tutorial, we'll learn how to:\n",
        "\n",
        "1.  Install the required dependencies.\n",
        "2.  Author a simple image classifier model.\n",
        "3.  Export the model to ONNX format.\n",
        "4.  Save the ONNX model in a file.\n",
        "5.  Visualize the ONNX model graph using\n",
        "    [Netron](https://github.com/lutzroeder/netron).\n",
        "6.  Execute the ONNX model with [ONNX Runtime]{.title-ref}\n",
        "7.  Compare the PyTorch results with the ones from the ONNX Runtime.\n",
        "\n",
        "1. Install the required dependencies\n",
        "====================================\n",
        "\n",
        "Because the ONNX exporter uses `onnx` and `onnxscript` to translate\n",
        "PyTorch operators into ONNX operators, we will need to install them.\n",
        "\n",
        "> ``` {.bash}\n",
        "> pip install onnx\n",
        "> pip install onnxscript\n",
        "> ```\n",
        "\n",
        "2. Author a simple image classifier model\n",
        "=========================================\n",
        "\n",
        "Once your environment is set up, let's start modeling our image\n",
        "classifier with PyTorch, exactly like we did in the [60 Minute\n",
        "Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx\n",
        "!pip install onnxscript"
      ],
      "metadata": {
        "id": "oXXmUvz-3E9x",
        "outputId": "719c97d8-d9fb-42dc-f431-206911c38b50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.25.5)\n",
            "Requirement already satisfied: onnxscript in /usr/local/lib/python3.10/dist-packages (0.1.0.dev20241122)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnxscript) (1.26.4)\n",
            "Requirement already satisfied: onnx>=1.16 in /usr/local/lib/python3.10/dist-packages (from onnxscript) (1.17.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from onnxscript) (4.12.2)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.10/dist-packages (from onnxscript) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxscript) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.16->onnxscript) (4.25.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JbwrcbCU3CrG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df-Wu-LW3CrH"
      },
      "source": [
        "3. Export the model to ONNX format\n",
        "==================================\n",
        "\n",
        "Now that we have our model defined, we need to instantiate it and create\n",
        "a random 32x32 input. Next, we can export the model to ONNX format.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from collections import OrderedDict\n",
        "\n",
        "def count_params(a):\n",
        "    out = sum(p.numel() for p in a.parameters())\n",
        "    return out\n",
        "\n",
        "#Resnet\n",
        "ENCODER_RESNET = [\n",
        "    'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152',\n",
        "    'resnext50_32x4d', 'resnext101_32x8d'\n",
        "]\n",
        "class Resnet(nn.Module):\n",
        "    def __init__(self, backbone='resnet50', pretrained=True):\n",
        "        super(Resnet, self).__init__()\n",
        "        assert backbone in ENCODER_RESNET\n",
        "        self.encoder = getattr(models, backbone)(pretrained=pretrained)\n",
        "        del self.encoder.fc, self.encoder.avgpool\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        x = self.encoder.conv1(x)\n",
        "        x = self.encoder.bn1(x)\n",
        "        x = self.encoder.relu(x)\n",
        "        x = self.encoder.maxpool(x)\n",
        "\n",
        "        x = self.encoder.layer1(x);  features.append(x)  # 1/4\n",
        "        x = self.encoder.layer2(x);  features.append(x)  # 1/8\n",
        "        x = self.encoder.layer3(x);  features.append(x)  # 1/16\n",
        "        x = self.encoder.layer4(x);  features.append(x)  # 1/32\n",
        "        return features\n",
        "\n",
        "    def list_blocks(self):\n",
        "        lst = [m for m in self.encoder.children()]\n",
        "        block0 = lst[:4]\n",
        "        block1 = lst[4:5]\n",
        "        block2 = lst[5:6]\n",
        "        block3 = lst[6:7]\n",
        "        block4 = lst[7:8]\n",
        "        return block0, block1, block2, block3, block4\n",
        "\n",
        "class CircConv(nn.Module):\n",
        "    def __init__(self,in_depth,out_depth, AF='prelu',BN=False,ks=3):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.in_depth = in_depth\n",
        "        self.out_depth = out_depth\n",
        "        self.AF = AF\n",
        "        self.BN = BN\n",
        "        #Layers\n",
        "        self.Conv = nn.Conv2d(self.in_depth,self.out_depth,ks,1,ks//2,padding_mode='circular')\n",
        "        self.prelu = nn.PReLU(self.out_depth)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.bn = nn.BatchNorm2d(self.out_depth)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.Conv(x)\n",
        "        if self.BN:\n",
        "            out = self.bn(out)\n",
        "        if self.AF == 'relu':\n",
        "            out = self.relu(out)\n",
        "        elif self.AF == 'prelu':\n",
        "            out = self.prelu(out)\n",
        "        elif self.AF == 'sigmoid':\n",
        "            out = self.sigmoid(out)\n",
        "        else:\n",
        "            out = out\n",
        "        return out\n",
        "\n",
        "#Initial Convolutional block\n",
        "class WConv(nn.Module):\n",
        "    def __init__(self,in_depth, out_depth, AF=None, BN=False, p=0.0):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.in_depth = in_depth\n",
        "        self.out_depth = out_depth\n",
        "        #Layers\n",
        "        self.layer1 = nn.Sequential(CircConv(self.in_depth,self.in_depth//4     ,AF=AF,BN=BN,ks=1),\n",
        "                                    nn.Dropout2d(p),\n",
        "                                    CircConv(self.in_depth//4,self.in_depth//4  ,AF=AF,BN=BN,ks=3),\n",
        "                                    nn.Dropout2d(p),\n",
        "                                    CircConv(self.in_depth//4,self.out_depth    ,AF=AF,BN=BN,ks=1))\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.layer1(x)\n",
        "        return out\n",
        "\n",
        "class Spectra(nn.Module):\n",
        "    def __init__(self,in_depth,AF='prelu'):\n",
        "        super().__init__()\n",
        "\n",
        "        #Params\n",
        "        self.in_depth = in_depth\n",
        "        self.inter_depth = self.in_depth//2 if in_depth>=2 else self.in_depth\n",
        "\n",
        "        #Layers\n",
        "        self.AF1 = nn.ReLU if AF=='relu' else nn.PReLU(self.inter_depth)\n",
        "        self.AF2 = nn.ReLU if AF=='relu' else nn.PReLU(self.inter_depth)\n",
        "        self.inConv = nn.Sequential(nn.Conv2d(self.in_depth,self.inter_depth,1),\n",
        "                                    nn.BatchNorm2d(self.inter_depth),\n",
        "                                    self.AF1)\n",
        "        self.midConv = nn.Sequential(nn.Conv2d(self.inter_depth,self.inter_depth,1),\n",
        "                                    nn.BatchNorm2d(self.inter_depth),\n",
        "                                    self.AF2)\n",
        "        self.outConv = nn.Conv2d(self.inter_depth, self.in_depth, 1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.inConv(x)\n",
        "        skip = x.clone()\n",
        "        rfft = torch.fft.rfft2(x)\n",
        "        real_rfft = torch.real(rfft)\n",
        "        imag_rfft = torch.imag(rfft)\n",
        "        cat_rfft = torch.cat((real_rfft,imag_rfft),dim=-1)\n",
        "        cat_rfft = self.midConv(cat_rfft)\n",
        "        mid = cat_rfft.shape[-1]//2\n",
        "        real_rfft = cat_rfft[...,:mid]\n",
        "        imag_rfft = cat_rfft[...,mid:]\n",
        "        rfft = torch.complex(real_rfft,imag_rfft)\n",
        "        spect = torch.fft.irfft2(rfft)\n",
        "        out = self.outConv(spect + skip)\n",
        "        return out\n",
        "\n",
        "class FastFC(nn.Module):\n",
        "    def __init__(self,in_depth,AF='prelu'):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.in_depth = in_depth//2\n",
        "\n",
        "        #Layers\n",
        "        self.AF1 = nn.ReLU if AF=='relu' else nn.PReLU(self.in_depth)\n",
        "        self.AF2 = nn.ReLU if AF=='relu' else nn.PReLU(self.in_depth)\n",
        "        self.conv_ll = nn.Conv2d(self.in_depth,self.in_depth,3,padding='same')\n",
        "        self.conv_lg = nn.Conv2d(self.in_depth,self.in_depth,3,padding='same')\n",
        "        self.conv_gl = nn.Conv2d(self.in_depth,self.in_depth,3,padding='same')\n",
        "        self.conv_gg = Spectra(self.in_depth, AF)\n",
        "        self.bnaf1 = nn.Sequential(nn.BatchNorm2d(self.in_depth),self.AF1)\n",
        "        self.bnaf2 = nn.Sequential(nn.BatchNorm2d(self.in_depth),self.AF2)\n",
        "\n",
        "    def forward(self,x):\n",
        "        mid = x.shape[1]//2\n",
        "        x_loc = x[:,:mid,:,:]\n",
        "        x_glo = x[:,mid:,:,:]\n",
        "        x_ll = self.conv_ll(x_loc)\n",
        "        x_lg = self.conv_lg(x_loc)\n",
        "        x_gl = self.conv_gl(x_glo)\n",
        "        x_gg = self.conv_gg(x_glo)\n",
        "        out_loc = torch.add((self.bnaf1(x_ll + x_gl)),x_loc)\n",
        "        out_glo = torch.add((self.bnaf2(x_gg + x_lg)),x_glo)\n",
        "        out = torch.cat((out_loc,out_glo),dim=1)\n",
        "        return out,out_loc,out_glo\n",
        "\n",
        "class FourierBlock(nn.Module):\n",
        "    def __init__(self,num_layer,in_depth,return_all=True):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.num_layers = num_layer\n",
        "        self.in_depth = in_depth\n",
        "        self.return_all = return_all\n",
        "        #Layers\n",
        "        self.block = nn.ModuleList()\n",
        "        for _ in range(self.num_layers):\n",
        "            self.block.append(FastFC(self.in_depth,'prelu'))\n",
        "\n",
        "    def forward(self,x):\n",
        "        for layer in self.block:\n",
        "            x,x_loc,x_glo = layer(x)\n",
        "        if self.return_all:\n",
        "            return x,x_loc,x_glo\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,features_depth,latent_depth):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.features_depth = features_depth\n",
        "        self.num_maps = len(features_depth)\n",
        "        self.latent_depth = latent_depth\n",
        "\n",
        "        #Layers\n",
        "        self.FB1 = FourierBlock(1,self.features_depth[0],False)\n",
        "        self.down1 = nn.Upsample(scale_factor=0.5,mode='bilinear',align_corners=False)\n",
        "        self.convB1 = WConv(self.features_depth[0],self.features_depth[1],AF='prelu',BN=True, p=0.4)\n",
        "        self.FB2 = FourierBlock(1,self.features_depth[1],False)\n",
        "        self.down2 = nn.Upsample(scale_factor=0.5,mode='bilinear',align_corners=False)\n",
        "        self.convB2 = WConv(self.features_depth[1],self.features_depth[2],AF='prelu',BN=True, p=0.4)\n",
        "        self.FB3 = FourierBlock(1,self.features_depth[2],False)\n",
        "        self.down3 = nn.Upsample(scale_factor=0.5,mode='bilinear',align_corners=False)\n",
        "        self.convB3 = WConv(self.features_depth[2],self.features_depth[3],AF='prelu',BN=True, p=0.4)\n",
        "        self.FB4 = FourierBlock(1,self.features_depth[3],False)\n",
        "        self.convB4 = WConv(self.features_depth[3],self.latent_depth     ,AF='prelu',BN=True, p=0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        flow = x[0]\n",
        "        f1 = self.FB1(flow)\n",
        "        flow = self.convB1(self.down1(f1)) + x[1]  # Use out-of-place addition\n",
        "        f2 = self.FB2(flow)\n",
        "        flow = self.convB2(self.down2(f2)) + x[2]  # Use out-of-place addition\n",
        "        f3 = self.FB3(flow)\n",
        "        flow = self.convB3(self.down3(f3)) + x[3]  # Use out-of-place addition\n",
        "        f4 = self.FB4(flow)\n",
        "        out = self.convB4(f4)\n",
        "        inter_features = [f4, f3, f2, f1]\n",
        "        return out, inter_features\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,latent_depth,feat_depth):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.in_depth = latent_depth\n",
        "        self.feat_depth = feat_depth\n",
        "\n",
        "        #Layers\n",
        "        self.convB4 = WConv(self.in_depth,self.feat_depth[0],AF='prelu',BN=True)\n",
        "        self.alpha4 = nn.Parameter(torch.randn((1)))\n",
        "        self.FB4 = FourierBlock(1,self.feat_depth[0],False)\n",
        "\n",
        "        self.convB3 = WConv(self.feat_depth[0],self.feat_depth[1],AF='prelu',BN=True)\n",
        "        self.up3 = nn.Upsample(scale_factor=2,mode='bilinear',align_corners=False)\n",
        "        self.alpha3 = nn.Parameter(torch.randn((1)))\n",
        "        self.FB3 = FourierBlock(1,self.feat_depth[1],False)\n",
        "\n",
        "        self.convB2 = WConv(self.feat_depth[1],self.feat_depth[2],AF='prelu',BN=True)\n",
        "        self.up2 = nn.Upsample(scale_factor=2,mode='bilinear',align_corners=False)\n",
        "        self.alpha2 = nn.Parameter(torch.randn((1)))\n",
        "        self.FB2 = FourierBlock(1,self.feat_depth[2],False)\n",
        "\n",
        "        self.convB1 = WConv(self.feat_depth[2],self.feat_depth[3],AF='prelu',BN=True)\n",
        "        self.up1 = nn.Upsample(scale_factor=2,mode='bilinear',align_corners=False)\n",
        "        self.alpha1 = nn.Parameter(torch.randn((1)))\n",
        "        self.FB1 = FourierBlock(1,self.feat_depth[3],False)\n",
        "\n",
        "        self.U1 = nn.Sequential(WConv(self.feat_depth[3],self.feat_depth[4],AF='prelu',BN=True),\n",
        "                                nn.Upsample(scale_factor=2,mode='bilinear',align_corners=False),\n",
        "                                FourierBlock(1,self.feat_depth[4],False))\n",
        "\n",
        "        self.U2 = nn.Sequential(WConv(self.feat_depth[4],self.feat_depth[5],AF='prelu',BN=True),\n",
        "                                nn.Upsample(scale_factor=2,mode='bilinear',align_corners=False),\n",
        "                                FourierBlock(1,feat_depth[5],False))\n",
        "\n",
        "    def forward(self,x,inter_features):\n",
        "        x = torch.add(self.convB4(x),inter_features[0]*self.alpha4[0])\n",
        "        f1 = self.FB4(x)\n",
        "        x = torch.add(self.up3(self.convB3(f1)),inter_features[1]*self.alpha3[0])\n",
        "        f2 = self.FB3(x)\n",
        "        x = torch.add(self.up2(self.convB2(f2)),inter_features[2]*self.alpha2[0])\n",
        "        f3 = self.FB2(x)\n",
        "        x = torch.add(self.up1(self.convB1(f3)),inter_features[3]*self.alpha1[0])\n",
        "        f4 = self.FB1(x)\n",
        "        f5 = self.U1(f4)\n",
        "        f6 = self.U2(f5)\n",
        "        upscale_features = [f1,f2,f3,f4,f5,f6]\n",
        "        return upscale_features\n",
        "\n",
        "class SemanticBranch(nn.Module):\n",
        "    def __init__(self,inter_depth,num_classes,features_depth):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.inter_depth = inter_depth\n",
        "        self.num_classes = num_classes\n",
        "        self.feat_depths = features_depth\n",
        "        self.num_feat_maps = len(self.feat_depths)\n",
        "        #Layers\n",
        "        self.alphas = nn.ParameterList()\n",
        "        self.ScaleMediator = nn.ModuleList()\n",
        "        for i in range(self.num_feat_maps):\n",
        "            self.alphas.append(nn.Parameter(torch.randn(1)))\n",
        "            self.ScaleMediator.append(nn.Sequential(WConv(self.feat_depths[i],self.inter_depth,AF='relu',BN=True),\n",
        "                                                    nn.UpsamplingBilinear2d(scale_factor=(2**(self.num_feat_maps-i-1)))))\n",
        "        self.outSemanticConv = CircConv(self.inter_depth, self.num_classes, AF='relu', BN=True, ks=3)\n",
        "\n",
        "    def forward(self,feat_list):\n",
        "        out = self.ScaleMediator[0](feat_list[0])\n",
        "        for i in range(1,self.num_feat_maps):\n",
        "            out += self.ScaleMediator[i](feat_list[i]) * self.alphas[i]\n",
        "        out = self.outSemanticConv(out)\n",
        "        return out\n",
        "\n",
        "class DepthBranch(nn.Module):\n",
        "    def __init__(self,inter_depth,features_depth):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.inter_depth = inter_depth\n",
        "        self.feat_depths = features_depth\n",
        "        self.num_feat_maps = len(self.feat_depths)\n",
        "        #Layers\n",
        "        self.alphas = nn.ParameterList()\n",
        "        self.ScaleMediator = nn.ModuleList()\n",
        "        for i in range(self.num_feat_maps):\n",
        "            self.alphas.append(nn.Parameter(torch.randn(1)))\n",
        "            self.ScaleMediator.append(nn.Sequential(WConv(self.feat_depths[i],self.inter_depth,AF='prelu',BN=True),\n",
        "                                                    nn.UpsamplingBilinear2d(scale_factor=(2**(self.num_feat_maps-i-1)))))\n",
        "\n",
        "        self.outDepthConv = CircConv(self.inter_depth,1,'relu',False)\n",
        "\n",
        "    def forward(self,feat_list):\n",
        "        out = self.ScaleMediator[0](feat_list[0])\n",
        "        for i in range(1,self.num_feat_maps):\n",
        "            out += self.ScaleMediator[i](feat_list[i]) * self.alphas[i]\n",
        "        out = self.outDepthConv(out)\n",
        "        return out\n",
        "\n",
        "class FDS(nn.Module):\n",
        "    ''' Main network body for semantic segmentation and depth estimation\n",
        "        from single panoramas (equirectangular for now) -> Use of EquiConvs?'''\n",
        "    x_mean = torch.FloatTensor(np.array([0.485,0.456,0.406])[None, :, None, None])\n",
        "    x_std  = torch.FloatTensor(np.array([0.229,0.224,0.225])[None, :, None, None])\n",
        "\n",
        "    def __init__(self,num_classes,backbone='resnet50'):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        ## Non-learnable parameters\n",
        "        self.latent_depth = 1024\n",
        "        self.num_classes = int(num_classes)\n",
        "        ## Architecture parameters\n",
        "        self.backbone = backbone\n",
        "        self.semantic_inter = 128\n",
        "        self.depth_inter = 128\n",
        "        ## Auxiliar data\n",
        "        self.features_depth = [256,512,1024,2048] #Depth from ResNet layers\n",
        "        self.feat_scale = [1/8,1/4,1/2,1]\n",
        "\n",
        "        #Layers\n",
        "        ##Encoder part\n",
        "        self.feature_extractor = Resnet(self.backbone) # resnet18, resnet50 Â¿resnet101?\n",
        "        self.Encoder = Encoder(self.features_depth,self.latent_depth)\n",
        "\n",
        "        ##Decoder part\n",
        "        self.decoder_depth = [2048,1024,512,256,64,16]\n",
        "        self.Decoder = Decoder(self.latent_depth,self.decoder_depth)\n",
        "\n",
        "        #Semantic branch\n",
        "        self.depmaps_seg = [1024,512,256,64,16]\n",
        "        self.SemanticSegmentator = SemanticBranch(self.semantic_inter,self.num_classes,self.depmaps_seg)\n",
        "\n",
        "        #Depth branch\n",
        "        self.depmaps_depth = [256,64,16]\n",
        "        self.DepthEstimator = DepthBranch(self.depth_inter,self.depmaps_depth)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self._prepare_x(x)\n",
        "        output = {}\n",
        "\n",
        "        #Feature extraction\n",
        "        feature_list = self.feature_extractor(x)\n",
        "        enc_features,inter_features = self.Encoder(feature_list)\n",
        "\n",
        "        #Decoder\n",
        "        upscale_features = self.Decoder(enc_features,inter_features)\n",
        "\n",
        "        #Depth prediction\n",
        "        output['Depth'] = self.DepthEstimator(upscale_features[-len(self.depmaps_depth):])\n",
        "\n",
        "        #Semantic branch\n",
        "        output['Semantic'] = self.SemanticSegmentator(upscale_features[-len(self.depmaps_seg):])\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _prepare_x(self, x):\n",
        "        if self.x_mean.device != x.device:\n",
        "            self.x_mean = self.x_mean.to(x.device)\n",
        "            self.x_std = self.x_std.to(x.device)\n",
        "        return (x[:, :3] - self.x_mean) / self.x_std\n",
        "\n",
        "    def param_count_sections(self):\n",
        "        bkb_params = count_params(self.feature_extractor)\n",
        "        enc_params = count_params(self.Encoder)\n",
        "        dec_params = count_params(self.Decoder)\n",
        "        seg_params = count_params(self.SemanticSegmentator)\n",
        "        dep_params = count_params(self.DepthEstimator)\n",
        "        all_params = np.sum([bkb_params,enc_params,dec_params,seg_params,dep_params])\n",
        "        print('Feature extractor parameters: %.2f M' %(bkb_params/1e6))\n",
        "        print('Encoder parameters: %.2f M' %(enc_params/1e6))\n",
        "        print('Decoder parameters: %.2f M' %(dec_params/1e6))\n",
        "        print('Segmentation branch parameters: %.2f M' %(seg_params/1e6))\n",
        "        print('Depth branch parameters: %.2f M' %(dep_params/1e6))\n",
        "        print('Total number of parameters: %.2f M' %(all_params/1e6))\n",
        "\n",
        "def save_weights(net,params,args,path):\n",
        "    state_dict = OrderedDict({\n",
        "                'args': args.__dict__,\n",
        "                'kargs': {\n",
        "                    'num_classes': params['num_classes'],\n",
        "                    'backbone': params['backbone']},\n",
        "                'state_dict': net.state_dict()})\n",
        "    torch.save(state_dict,path)\n",
        "\n",
        "def load_weigths(args):\n",
        "    state_dict = torch.load(args.pth,map_location='cpu')\n",
        "    net = FDS(**state_dict['kargs'])\n",
        "    try:\n",
        "        net.load_state_dict(state_dict['state_dict'])\n",
        "    except:\n",
        "        stt_dict = i3vea2local(state_dict['state_dict'])\n",
        "        net.load_state_dict(stt_dict)\n",
        "    return net,state_dict\n",
        "\n",
        "def i3vea2local(state_dict):\n",
        "    out_dict = OrderedDict([(k,v) for k,v in state_dict.items()])\n",
        "    return out_dict\n",
        "\n",
        "device = torch.device('cpu')\n",
        "batch = 1\n",
        "ch,h,w = 3,512,1024\n",
        "dummy = torch.rand((batch,ch,h,w))\n",
        "resnet = FDS(14).to(device)\n",
        "output = resnet(dummy.to(device))\n",
        "onnx_program = torch.onnx.dynamo_export(resnet, dummy)\n"
      ],
      "metadata": {
        "id": "VDOqRRuO3ahW",
        "outputId": "4ab528c4-67f6-4b52-ffda-cd69b621f34e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
            "  param_schemas = callee.param_schemas()\n",
            "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
            "  param_schemas = callee.param_schemas()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/_exporter_legacy.py:116: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OnnxExporterError",
          "evalue": "Failed to export the model to ONNX. Generating SARIF report at 'report_dynamo_export.sarif'. SARIF is a standard format for the output of static analysis tools. SARIF logs can be loaded in VS Code SARIF viewer extension, or SARIF web viewer (https://microsoft.github.io/sarif-web-component/). Please report a bug on PyTorch Github: https://github.com/pytorch/pytorch/issues",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/_exporter_legacy.py\u001b[0m in \u001b[0;36mdynamo_export\u001b[0;34m(model, export_options, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m             ).export()\n\u001b[0m\u001b[1;32m   1223\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/_exporter_legacy.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    975\u001b[0m         ), torch._dynamo.config.patch(dataclasses.asdict(DEFAULT_EXPORT_DYNAMO_CONFIG)):\n\u001b[0;32m--> 976\u001b[0;31m             graph_module = self.options.fx_tracer.generate_fx(\n\u001b[0m\u001b[1;32m    977\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/fx/dynamo_graph_extractor.py\u001b[0m in \u001b[0;36mgenerate_fx\u001b[0;34m(self, options, model, model_args, model_kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_export_passes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdated_model_args\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[return-value]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/fx/dynamo_graph_extractor.py\u001b[0m in \u001b[0;36mpre_export_passes\u001b[0;34m(self, options, original_model, fx_module, fx_module_args)\u001b[0m\n\u001b[1;32m    225\u001b[0m     ):\n\u001b[0;32m--> 226\u001b[0;31m         return _exporter_legacy.common_pre_export_passes(\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfx_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfx_module_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/_exporter_legacy.py\u001b[0m in \u001b[0;36mcommon_pre_export_passes\u001b[0;34m(options, original_model, fx_module, fx_module_args)\u001b[0m\n\u001b[1;32m   1257\u001b[0m     \u001b[0;31m# Functionalize to get a semantically equivalent graph without mutations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1258\u001b[0;31m     module = passes.Functionalize(\n\u001b[0m\u001b[1;32m   1259\u001b[0m         \u001b[0mdiagnostic_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/diagnostics/infra/decorator.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_and_raise_if_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/diagnostics/infra/context.py\u001b[0m in \u001b[0;36mlog_and_raise_if_error\u001b[0;34m(self, diagnostic)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdiagnostic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mdiagnostic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeErrorWithDiagnostic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiagnostic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/diagnostics/infra/decorator.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                     \u001b[0mreturn_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mdiag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_section\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Return values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/fx/_pass.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/fx/passes/functionalization.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfake_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munset_fake_temporarily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             graph_module = proxy_tensor.make_fx(\n\u001b[0m\u001b[1;32m    121\u001b[0m                 \u001b[0mfunctionalized_callable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mGraphModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmake_fx_tracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(self, f, *args)\u001b[0m\n\u001b[1;32m   2047\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_modes_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2048\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trace_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\u001b[0m in \u001b[0;36m_trace_inner\u001b[0;34m(self, f, *args)\u001b[0m\n\u001b[1;32m   2033\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx_tracer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2034\u001b[0;31m             t = dispatch_trace(\n\u001b[0m\u001b[1;32m   2035\u001b[0m                 \u001b[0mwrap_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx_tracer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\u001b[0m in \u001b[0;36mdispatch_trace\u001b[0;34m(root, tracer, concrete_args)\u001b[0m\n\u001b[1;32m   1126\u001b[0m ) -> GraphModule:\n\u001b[0;32m-> 1127\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcrete_args\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/_symbolic_trace.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0;34m\"output\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m                     \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m                     \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*proxies, **_unused)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_map_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_tensor_proxy_slot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(arg0)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/fx/passes/functionalization.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs_functional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/fx/passes/_utils.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfx_traceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_node_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/interpreter.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, initial_env, enable_io_processing, *args)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/interpreter.py\u001b[0m in \u001b[0;36mrun_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/interpreter.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, target, args, kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;31m# Execute the function and return the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/proxy_tensor.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_fn_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_fn_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: false INTERNAL ASSERT FAILED at \"aten/src/ATen/RegisterFunctionalization_2.cpp\":8320, please report a bug to PyTorch. mutating a non-functional tensor with a functional tensor is not allowed. Please ensure that all of your inputs are wrapped inside of a functionalize() call.\n\nWhile executing %_native_batch_norm_legit : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit.default](args = (%convolution, %_param_constant1, %_param_constant2, %_tensor_constant3, %_tensor_constant4, True, 0.1, 1e-05), kwargs = {})\nOriginal traceback:\n  File \"<ipython-input-2-600434328376>\", line 351, in forward\n    feature_list = self.feature_extractor(x)\n  File \"<ipython-input-2-600434328376>\", line 28, in forward\n    x = self.encoder.bn1(x)\n",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOnnxExporterError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-600434328376>\u001b[0m in \u001b[0;36m<cell line: 414>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0mresnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFDS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m \u001b[0monnx_program\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamo_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36mdynamo_export\u001b[0;34m(model, export_options, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exporter_legacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdynamo_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         return dynamo_export(\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/_exporter_legacy.py\u001b[0m in \u001b[0;36mdynamo_export\u001b[0;34m(model, export_options, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m             \u001b[0;34mf\"Please report a bug on PyTorch Github: {_PYTORCH_GITHUB_ISSUES_URL}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         )\n\u001b[0;32m-> 1233\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOnnxExporterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOnnxExporterError\u001b[0m: Failed to export the model to ONNX. Generating SARIF report at 'report_dynamo_export.sarif'. SARIF is a standard format for the output of static analysis tools. SARIF logs can be loaded in VS Code SARIF viewer extension, or SARIF web viewer (https://microsoft.github.io/sarif-web-component/). Please report a bug on PyTorch Github: https://github.com/pytorch/pytorch/issues"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cVu6j6qP3CrH",
        "outputId": "819e2cf2-166a-40d8-8e76-76892d5758e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
            "  param_schemas = callee.param_schemas()\n",
            "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
            "  param_schemas = callee.param_schemas()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/_exporter_legacy.py:116: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/fx/onnxfunction_dispatcher.py:503: FutureWarning: 'onnxscript.values.TracedOnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
            "  self.param_schema = self.onnxfunction.param_schemas()\n"
          ]
        }
      ],
      "source": [
        "torch_model = MyModel()\n",
        "torch_input = torch.randn(1, 1, 32, 32)\n",
        "onnx_program = torch.onnx.dynamo_export(torch_model, torch_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEEgfyDk3CrH"
      },
      "source": [
        "As we can see, we didn\\'t need any code change to the model. The\n",
        "resulting ONNX model is stored within `torch.onnx.ONNXProgram` as a\n",
        "binary protobuf file.\n",
        "\n",
        "4. Save the ONNX model in a file\n",
        "================================\n",
        "\n",
        "Although having the exported model loaded in memory is useful in many\n",
        "applications, we can save it to disk with the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9K-L1u03CrH"
      },
      "outputs": [],
      "source": [
        "onnx_program.save(\"my_image_classifier.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_dX9hXQ3CrI"
      },
      "source": [
        "You can load the ONNX file back into memory and check if it is well\n",
        "formed with the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8ri8hNQ3CrI"
      },
      "outputs": [],
      "source": [
        "import onnx\n",
        "onnx_model = onnx.load(\"my_image_classifier.onnx\")\n",
        "onnx.checker.check_model(onnx_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_Dg2tQ43CrI"
      },
      "source": [
        "5. Visualize the ONNX model graph using Netron\n",
        "==============================================\n",
        "\n",
        "Now that we have our model saved in a file, we can visualize it with\n",
        "[Netron](https://github.com/lutzroeder/netron). Netron can either be\n",
        "installed on macos, Linux or Windows computers, or run directly from the\n",
        "browser. Let\\'s try the web version by opening the following link:\n",
        "<https://netron.app/>.\n",
        "\n",
        "![image](https://pytorch.org/tutorials/_static/img/onnx/netron_web_ui.png){.align-center\n",
        "width=\"70.0%\"}\n",
        "\n",
        "Once Netron is open, we can drag and drop our `my_image_classifier.onnx`\n",
        "file into the browser or select it after clicking the **Open model**\n",
        "button.\n",
        "\n",
        "![image](https://pytorch.org/tutorials/_static/img/onnx/image_clossifier_onnx_modelon_netron_web_ui.png){width=\"50.0%\"}\n",
        "\n",
        "And that is it! We have successfully exported our PyTorch model to ONNX\n",
        "format and visualized it with Netron.\n",
        "\n",
        "6. Execute the ONNX model with ONNX Runtime\n",
        "===========================================\n",
        "\n",
        "The last step is executing the ONNX model with [ONNX\n",
        "Runtime]{.title-ref}, but before we do that, let\\'s install ONNX\n",
        "Runtime.\n",
        "\n",
        "> ``` {.bash}\n",
        "> pip install onnxruntime\n",
        "> ```\n",
        "\n",
        "The ONNX standard does not support all the data structure and types that\n",
        "PyTorch does, so we need to adapt PyTorch input\\'s to ONNX format before\n",
        "feeding it to ONNX Runtime. In our example, the input happens to be the\n",
        "same, but it might have more inputs than the original PyTorch model in\n",
        "more complex models.\n",
        "\n",
        "ONNX Runtime requires an additional step that involves converting all\n",
        "PyTorch tensors to Numpy (in CPU) and wrap them on a dictionary with\n",
        "keys being a string with the input name as key and the numpy tensor as\n",
        "the value.\n",
        "\n",
        "Now we can create an *ONNX Runtime Inference Session*, execute the ONNX\n",
        "model with the processed input and get the output. In this tutorial,\n",
        "ONNX Runtime is executed on CPU, but it could be executed on GPU as\n",
        "well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hSMWz9-3CrJ"
      },
      "outputs": [],
      "source": [
        "import onnxruntime\n",
        "\n",
        "onnx_input = onnx_program.adapt_torch_inputs_to_onnx(torch_input)\n",
        "print(f\"Input length: {len(onnx_input)}\")\n",
        "print(f\"Sample input: {onnx_input}\")\n",
        "\n",
        "ort_session = onnxruntime.InferenceSession(\"./my_image_classifier.onnx\", providers=['CPUExecutionProvider'])\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "onnxruntime_input = {k.name: to_numpy(v) for k, v in zip(ort_session.get_inputs(), onnx_input)}\n",
        "\n",
        "onnxruntime_outputs = ort_session.run(None, onnxruntime_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akP-BljC3CrJ"
      },
      "source": [
        "7. Compare the PyTorch results with the ones from the ONNX Runtime\n",
        "==================================================================\n",
        "\n",
        "The best way to determine whether the exported model is looking good is\n",
        "through numerical evaluation against PyTorch, which is our source of\n",
        "truth.\n",
        "\n",
        "For that, we need to execute the PyTorch model with the same input and\n",
        "compare the results with ONNX Runtime\\'s. Before comparing the results,\n",
        "we need to convert the PyTorch\\'s output to match ONNX\\'s format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSKPYUZ83CrJ"
      },
      "outputs": [],
      "source": [
        "torch_outputs = torch_model(torch_input)\n",
        "torch_outputs = onnx_program.adapt_torch_outputs_to_onnx(torch_outputs)\n",
        "\n",
        "assert len(torch_outputs) == len(onnxruntime_outputs)\n",
        "for torch_output, onnxruntime_output in zip(torch_outputs, onnxruntime_outputs):\n",
        "    torch.testing.assert_close(torch_output, torch.tensor(onnxruntime_output))\n",
        "\n",
        "print(\"PyTorch and ONNX Runtime output matched!\")\n",
        "print(f\"Output length: {len(onnxruntime_outputs)}\")\n",
        "print(f\"Sample output: {onnxruntime_outputs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF9sIjo73CrM"
      },
      "source": [
        "Conclusion\n",
        "==========\n",
        "\n",
        "That is about it! We have successfully exported our PyTorch model to\n",
        "ONNX format, saved the model to disk, viewed it using Netron, executed\n",
        "it with ONNX Runtime and finally compared its numerical results with\n",
        "PyTorch\\'s.\n",
        "\n",
        "Further reading\n",
        "===============\n",
        "\n",
        "The list below refers to tutorials that ranges from basic examples to\n",
        "advanced scenarios, not necessarily in the order they are listed. Feel\n",
        "free to jump directly to specific topics of your interest or sit tight\n",
        "and have fun going through all of them to learn all there is about the\n",
        "ONNX exporter.\n",
        "\n",
        "::: {.toctree hidden=\"\"}\n",
        ":::\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}