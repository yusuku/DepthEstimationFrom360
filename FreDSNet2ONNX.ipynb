{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvqkNpX0nhzAbW/zZjTYAm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yusuku/DepthEstimationFrom360/blob/main/FreDSNet2ONNX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxscript\n",
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOQZMldjTM_b",
        "outputId": "c6405aaf-8a84-4bde-953e-5311424f2bac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxscript\n",
            "  Downloading onnxscript-0.1.0.dev20241123-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnxscript) (1.26.4)\n",
            "Collecting onnx>=1.16 (from onnxscript)\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from onnxscript) (4.12.2)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.10/dist-packages (from onnxscript) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxscript) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.16->onnxscript) (4.25.5)\n",
            "Downloading onnxscript-0.1.0.dev20241123-py3-none-any.whl (722 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.1/722.1 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, onnxscript\n",
            "Successfully installed onnx-1.17.0 onnxscript-0.1.0.dev20241123\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.25.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from collections import OrderedDict\n",
        "from torchvision.models import ResNet50_Weights"
      ],
      "metadata": {
        "id": "UDNuvL3KV86C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "swczye8X6aqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_params(a):\n",
        "    out = sum(p.numel() for p in a.parameters())\n",
        "    return out\n",
        "\n",
        "\n",
        "class Resnet(nn.Module):\n",
        "    def __init__(self, backbone='resnet50', pretrained=True):\n",
        "        super(Resnet, self).__init__()\n",
        "\n",
        "        self.encoder = getattr(models, backbone)(pretrained=pretrained)\n",
        "        del self.encoder.fc, self.encoder.avgpool\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        x = self.encoder.conv1(x)\n",
        "        x = self.encoder.bn1(x)\n",
        "        x = self.encoder.relu(x)\n",
        "        x = self.encoder.maxpool(x)\n",
        "\n",
        "        x = self.encoder.layer1(x);  features.append(x)  # 1/4\n",
        "        x = self.encoder.layer2(x);  features.append(x)  # 1/8\n",
        "        x = self.encoder.layer3(x);  features.append(x)  # 1/16\n",
        "        x = self.encoder.layer4(x);  features.append(x)  # 1/32\n",
        "        return features\n",
        "\n",
        "    def list_blocks(self):\n",
        "        lst = [m for m in self.encoder.children()]\n",
        "        block0 = lst[:4]\n",
        "        block1 = lst[4:5]\n",
        "        block2 = lst[5:6]\n",
        "        block3 = lst[6:7]\n",
        "        block4 = lst[7:8]\n",
        "        return block0, block1, block2, block3, block4\n",
        "\n",
        "class CircConv(nn.Module):\n",
        "    def __init__(self,in_depth,out_depth, AF='prelu',BN=False,ks=3):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.in_depth = in_depth\n",
        "        self.out_depth = out_depth\n",
        "        self.AF = AF\n",
        "        self.BN = BN\n",
        "        #Layers\n",
        "        self.Conv = nn.Conv2d(self.in_depth,self.out_depth,ks,1,ks//2,padding_mode='circular')\n",
        "        self.prelu = nn.PReLU(self.out_depth)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.bn = nn.BatchNorm2d(self.out_depth)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.Conv(x)\n",
        "        if self.BN:\n",
        "            out = self.bn(out)\n",
        "        if self.AF == 'relu':\n",
        "            out = self.relu(out)\n",
        "        elif self.AF == 'prelu':\n",
        "            out = self.prelu(out)\n",
        "        elif self.AF == 'sigmoid':\n",
        "            out = self.sigmoid(out)\n",
        "        else:\n",
        "            out = out\n",
        "        return out\n",
        "\n",
        "#Initial Convolutional block\n",
        "class WConv(nn.Module):\n",
        "    def __init__(self,in_depth, out_depth, AF=None, BN=False, p=0.0):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.in_depth = in_depth\n",
        "        self.out_depth = out_depth\n",
        "        #Layers\n",
        "        self.layer1 = nn.Sequential(CircConv(self.in_depth,self.in_depth//4     ,AF=AF,BN=BN,ks=1),\n",
        "                                    nn.Dropout2d(p),\n",
        "                                    CircConv(self.in_depth//4,self.in_depth//4  ,AF=AF,BN=BN,ks=3),\n",
        "                                    nn.Dropout2d(p),\n",
        "                                    CircConv(self.in_depth//4,self.out_depth    ,AF=AF,BN=BN,ks=1))\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.layer1(x)\n",
        "        return out\n",
        "\n",
        "class Spectra(nn.Module):\n",
        "    def __init__(self,in_depth,AF='prelu'):\n",
        "        super().__init__()\n",
        "\n",
        "        #Params\n",
        "        self.in_depth = in_depth\n",
        "        self.inter_depth = self.in_depth//2 if in_depth>=2 else self.in_depth\n",
        "\n",
        "        #Layers\n",
        "        self.AF1 = nn.ReLU if AF=='relu' else nn.PReLU(self.inter_depth)\n",
        "        self.AF2 = nn.ReLU if AF=='relu' else nn.PReLU(self.inter_depth)\n",
        "        self.inConv = nn.Sequential(nn.Conv2d(self.in_depth,self.inter_depth,1),\n",
        "                                    nn.BatchNorm2d(self.inter_depth),\n",
        "                                    self.AF1)\n",
        "        self.midConv = nn.Sequential(nn.Conv2d(self.inter_depth,self.inter_depth,1),\n",
        "                                    nn.BatchNorm2d(self.inter_depth),\n",
        "                                    self.AF2)\n",
        "        self.outConv = nn.Conv2d(self.inter_depth, self.in_depth, 1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.inConv(x)\n",
        "        skip = x.clone()\n",
        "        rfft = torch.fft.rfft2(x)\n",
        "        real_rfft = torch.real(rfft)\n",
        "        imag_rfft = torch.imag(rfft)\n",
        "        cat_rfft = torch.cat((real_rfft,imag_rfft),dim=-1)\n",
        "        cat_rfft = self.midConv(cat_rfft)\n",
        "        mid = cat_rfft.shape[-1]//2\n",
        "        real_rfft = cat_rfft[...,:mid]\n",
        "        imag_rfft = cat_rfft[...,mid:]\n",
        "        rfft = torch.complex(real_rfft,imag_rfft)\n",
        "        spect = torch.fft.irfft2(rfft)\n",
        "        out = self.outConv(spect + skip)\n",
        "        return out\n",
        "\n",
        "class FastFC(nn.Module):\n",
        "    def __init__(self,in_depth,AF='prelu'):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.in_depth = in_depth//2\n",
        "\n",
        "        #Layers\n",
        "        self.AF1 = nn.ReLU if AF=='relu' else nn.PReLU(self.in_depth)\n",
        "        self.AF2 = nn.ReLU if AF=='relu' else nn.PReLU(self.in_depth)\n",
        "        self.conv_ll = nn.Conv2d(self.in_depth,self.in_depth,3,padding='same')\n",
        "        self.conv_lg = nn.Conv2d(self.in_depth,self.in_depth,3,padding='same')\n",
        "        self.conv_gl = nn.Conv2d(self.in_depth,self.in_depth,3,padding='same')\n",
        "        self.conv_gg = Spectra(self.in_depth, AF)\n",
        "        self.bnaf1 = nn.Sequential(nn.BatchNorm2d(self.in_depth),self.AF1)\n",
        "        self.bnaf2 = nn.Sequential(nn.BatchNorm2d(self.in_depth),self.AF2)\n",
        "\n",
        "    def forward(self,x):\n",
        "        mid = x.shape[1]//2\n",
        "        x_loc = x[:,:mid,:,:]\n",
        "        x_glo = x[:,mid:,:,:]\n",
        "        x_ll = self.conv_ll(x_loc)\n",
        "        x_lg = self.conv_lg(x_loc)\n",
        "        x_gl = self.conv_gl(x_glo)\n",
        "        x_gg = self.conv_gg(x_glo)\n",
        "        out_loc = torch.add((self.bnaf1(x_ll + x_gl)),x_loc)\n",
        "        out_glo = torch.add((self.bnaf2(x_gg + x_lg)),x_glo)\n",
        "        out = torch.cat((out_loc,out_glo),dim=1)\n",
        "        return out,out_loc,out_glo\n",
        "\n",
        "class FourierBlock(nn.Module):\n",
        "    def __init__(self,num_layer,in_depth,return_all=True):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.num_layers = num_layer\n",
        "        self.in_depth = in_depth\n",
        "        self.return_all = return_all\n",
        "        #Layers\n",
        "        self.block = nn.ModuleList()\n",
        "        for _ in range(self.num_layers):\n",
        "            self.block.append(FastFC(self.in_depth,'prelu'))\n",
        "\n",
        "    def forward(self,x):\n",
        "        for layer in self.block:\n",
        "            x,x_loc,x_glo = layer(x)\n",
        "        if self.return_all:\n",
        "            return x,x_loc,x_glo\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,features_depth,latent_depth):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.features_depth = features_depth\n",
        "        self.num_maps = len(features_depth)\n",
        "        self.latent_depth = latent_depth\n",
        "\n",
        "        #Layers\n",
        "        self.FB1 = FourierBlock(1,self.features_depth[0],False)\n",
        "        self.down1 = nn.Upsample(scale_factor=0.5,mode='bilinear',align_corners=False)\n",
        "        self.convB1 = WConv(self.features_depth[0],self.features_depth[1],AF='prelu',BN=True, p=0.4)\n",
        "        self.FB2 = FourierBlock(1,self.features_depth[1],False)\n",
        "        self.down2 = nn.Upsample(scale_factor=0.5,mode='bilinear',align_corners=False)\n",
        "        self.convB2 = WConv(self.features_depth[1],self.features_depth[2],AF='prelu',BN=True, p=0.4)\n",
        "        self.FB3 = FourierBlock(1,self.features_depth[2],False)\n",
        "        self.down3 = nn.Upsample(scale_factor=0.5,mode='bilinear',align_corners=False)\n",
        "        self.convB3 = WConv(self.features_depth[2],self.features_depth[3],AF='prelu',BN=True, p=0.4)\n",
        "        self.FB4 = FourierBlock(1,self.features_depth[3],False)\n",
        "        self.convB4 = WConv(self.features_depth[3],self.latent_depth     ,AF='prelu',BN=True, p=0.4)\n",
        "\n",
        "    def forward(self,x):\n",
        "        flow = x[0]\n",
        "        f1 = self.FB1(flow)\n",
        "        flow = torch.add(self.convB1(self.down1(f1)),x[1])\n",
        "        f2 = self.FB2(flow)\n",
        "        flow = torch.add(self.convB2(self.down2(f2)),x[2])\n",
        "        f3 = self.FB3(flow)\n",
        "        flow = torch.add(self.convB3(self.down3(f3)),x[3])\n",
        "        f4 = self.FB4(flow)\n",
        "        out = self.convB4(f4)\n",
        "        inter_features = [f4,f3,f2,f1]\n",
        "        return out,inter_features\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,latent_depth,feat_depth):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.in_depth = latent_depth\n",
        "        self.feat_depth = feat_depth\n",
        "\n",
        "        #Layers\n",
        "        self.convB4 = WConv(self.in_depth,self.feat_depth[0],AF='prelu',BN=True)\n",
        "        self.alpha4 = nn.Parameter(torch.randn((1)))\n",
        "        self.FB4 = FourierBlock(1,self.feat_depth[0],False)\n",
        "\n",
        "        self.convB3 = WConv(self.feat_depth[0],self.feat_depth[1],AF='prelu',BN=True)\n",
        "        self.up3 = nn.Upsample(scale_factor=2,mode='bilinear',align_corners=False)\n",
        "        self.alpha3 = nn.Parameter(torch.randn((1)))\n",
        "        self.FB3 = FourierBlock(1,self.feat_depth[1],False)\n",
        "\n",
        "        self.convB2 = WConv(self.feat_depth[1],self.feat_depth[2],AF='prelu',BN=True)\n",
        "        self.up2 = nn.Upsample(scale_factor=2,mode='bilinear',align_corners=False)\n",
        "        self.alpha2 = nn.Parameter(torch.randn((1)))\n",
        "        self.FB2 = FourierBlock(1,self.feat_depth[2],False)\n",
        "\n",
        "        self.convB1 = WConv(self.feat_depth[2],self.feat_depth[3],AF='prelu',BN=True)\n",
        "        self.up1 = nn.Upsample(scale_factor=2,mode='bilinear',align_corners=False)\n",
        "        self.alpha1 = nn.Parameter(torch.randn((1)))\n",
        "        self.FB1 = FourierBlock(1,self.feat_depth[3],False)\n",
        "\n",
        "        self.U1 = nn.Sequential(WConv(self.feat_depth[3],self.feat_depth[4],AF='prelu',BN=True),\n",
        "                                nn.Upsample(scale_factor=2,mode='bilinear',align_corners=False),\n",
        "                                FourierBlock(1,self.feat_depth[4],False))\n",
        "\n",
        "        self.U2 = nn.Sequential(WConv(self.feat_depth[4],self.feat_depth[5],AF='prelu',BN=True),\n",
        "                                nn.Upsample(scale_factor=2,mode='bilinear',align_corners=False),\n",
        "                                FourierBlock(1,feat_depth[5],False))\n",
        "\n",
        "    def forward(self,x,inter_features):\n",
        "        x = torch.add(self.convB4(x),inter_features[0]*self.alpha4[0])\n",
        "        f1 = self.FB4(x)\n",
        "        x = torch.add(self.up3(self.convB3(f1)),inter_features[1]*self.alpha3[0])\n",
        "        f2 = self.FB3(x)\n",
        "        x = torch.add(self.up2(self.convB2(f2)),inter_features[2]*self.alpha2[0])\n",
        "        f3 = self.FB2(x)\n",
        "        x = torch.add(self.up1(self.convB1(f3)),inter_features[3]*self.alpha1[0])\n",
        "        f4 = self.FB1(x)\n",
        "        f5 = self.U1(f4)\n",
        "        f6 = self.U2(f5)\n",
        "        upscale_features = [f1,f2,f3,f4,f5,f6]\n",
        "        return upscale_features\n",
        "\n",
        "class SemanticBranch(nn.Module):\n",
        "    def __init__(self,inter_depth,num_classes,features_depth):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.inter_depth = inter_depth\n",
        "        self.num_classes = num_classes\n",
        "        self.feat_depths = features_depth\n",
        "        self.num_feat_maps = len(self.feat_depths)\n",
        "        #Layers\n",
        "        self.alphas = nn.ParameterList()\n",
        "        self.ScaleMediator = nn.ModuleList()\n",
        "        for i in range(self.num_feat_maps):\n",
        "            self.alphas.append(nn.Parameter(torch.randn(1)))\n",
        "            self.ScaleMediator.append(nn.Sequential(WConv(self.feat_depths[i],self.inter_depth,AF='relu',BN=True),\n",
        "                                                    nn.UpsamplingBilinear2d(scale_factor=(2**(self.num_feat_maps-i-1)))))\n",
        "        self.outSemanticConv = CircConv(self.inter_depth, self.num_classes, AF='relu', BN=True, ks=3)\n",
        "\n",
        "    def forward(self,feat_list):\n",
        "        out = self.ScaleMediator[0](feat_list[0])\n",
        "        for i in range(1,self.num_feat_maps):\n",
        "            out += self.ScaleMediator[i](feat_list[i]) * self.alphas[i]\n",
        "        out = self.outSemanticConv(out)\n",
        "        return out\n",
        "\n",
        "class DepthBranch(nn.Module):\n",
        "    def __init__(self,inter_depth,features_depth):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.inter_depth = inter_depth\n",
        "        self.feat_depths = features_depth\n",
        "        self.num_feat_maps = len(self.feat_depths)\n",
        "        #Layers\n",
        "        self.alphas = nn.ParameterList()\n",
        "        self.ScaleMediator = nn.ModuleList()\n",
        "        for i in range(self.num_feat_maps):\n",
        "            self.alphas.append(nn.Parameter(torch.randn(1)))\n",
        "            self.ScaleMediator.append(nn.Sequential(WConv(self.feat_depths[i],self.inter_depth,AF='prelu',BN=True),\n",
        "                                                    nn.UpsamplingBilinear2d(scale_factor=(2**(self.num_feat_maps-i-1)))))\n",
        "\n",
        "        self.outDepthConv = CircConv(self.inter_depth,1,'relu',False)\n",
        "\n",
        "    def forward(self,feat_list):\n",
        "        out = self.ScaleMediator[0](feat_list[0])\n",
        "        for i in range(1,self.num_feat_maps):\n",
        "            out += self.ScaleMediator[i](feat_list[i]) * self.alphas[i]\n",
        "        out = self.outDepthConv(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def save_weights(net,params,args,path):\n",
        "    state_dict = OrderedDict({\n",
        "                'args': args.__dict__,\n",
        "                'kargs': {\n",
        "                    'num_classes': params['num_classes'],\n",
        "                    'backbone': params['backbone']},\n",
        "                'state_dict': net.state_dict()})\n",
        "    torch.save(state_dict,path)\n",
        "\n",
        "def load_weigths(args):\n",
        "    state_dict = torch.load(args.pth,map_location='cpu')\n",
        "    net = FDS(**state_dict['kargs'])\n",
        "    try:\n",
        "        net.load_state_dict(state_dict['state_dict'])\n",
        "    except:\n",
        "        stt_dict = i3vea2local(state_dict['state_dict'])\n",
        "        net.load_state_dict(stt_dict)\n",
        "    return net,state_dict\n",
        "\n",
        "def i3vea2local(state_dict):\n",
        "    out_dict = OrderedDict([(k,v) for k,v in state_dict.items()])\n",
        "    return out_dict"
      ],
      "metadata": {
        "id": "pMibpF2BXDoT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class FDS(nn.Module):\n",
        "    x_mean = torch.FloatTensor(np.array([0.485,0.456,0.406])[None, :, None, None])\n",
        "    x_std  = torch.FloatTensor(np.array([0.229,0.224,0.225])[None, :, None, None])\n",
        "\n",
        "\n",
        "    def __init__(self,num_classes,backbone='resnet50'):\n",
        "        super(FDS, self).__init__()\n",
        "        self.backbone= backbone\n",
        "        print(backbone)\n",
        "        self.feature_extractor = Resnet(self.backbone)\n",
        "\n",
        "        self.latent_depth = 1024\n",
        "        self.features_depth = [256,512,1024,2048] #Depth from ResNet layers\n",
        "        self.Encoder = Encoder(self.features_depth,self.latent_depth)\n",
        "\n",
        "        self.decoder_depth = [2048,1024,512,256,64,16]\n",
        "        self.Decoder = Decoder(self.latent_depth,self.decoder_depth)\n",
        "\n",
        "        self.depth_inter = 128\n",
        "        self.depmaps_depth = [256,64,16]\n",
        "        self.DepthEstimator = DepthBranch(self.depth_inter,self.depmaps_depth)\n",
        "\n",
        "        #Depth branch\n",
        "        self.num_classes = int(num_classes)\n",
        "        self.semantic_inter = 128\n",
        "        self.depmaps_seg = [1024,512,256,64,16]\n",
        "        self.SemanticSegmentator = SemanticBranch(self.semantic_inter,self.num_classes,self.depmaps_seg)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._prepare_x(x)\n",
        "        output = {}\n",
        "\n",
        "        feature_list = self.feature_extractor(x)\n",
        "        enc_features,inter_features = self.Encoder(feature_list)\n",
        "        upscale_features = self.Decoder(enc_features,inter_features)\n",
        "\n",
        "        #Depth prediction\n",
        "        output['Depth'] = self.DepthEstimator(upscale_features[-len(self.depmaps_depth):])\n",
        "\n",
        "        #Semantic branch\n",
        "        output['Semantic'] = self.SemanticSegmentator(upscale_features[-len(self.depmaps_seg):])\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _prepare_x(self, x):\n",
        "        if self.x_mean.device != x.device:\n",
        "            self.x_mean = self.x_mean.to(x.device)\n",
        "            self.x_std = self.x_std.to(x.device)\n",
        "        return (x[:, :3] - self.x_mean) / self.x_std\n",
        "torch_model = FDS(14)\n",
        "torch_input = torch.randn(1, 3, 512, 1024)\n",
        "torch_model.eval()\n",
        "onnx_program = torch.onnx.dynamo_export(torch_model, torch_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5hOc5dQWwxe",
        "outputId": "7265bd3f-9118-4386-be97-7f96a7d431d7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resnet50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/_exporter_legacy.py:116: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/fx/passes/readability.py:52: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
            "  new_node = self.module.graph.get_attr(normalized_name)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1586: UserWarning: Node feature_extractor_encoder_bn1_running_mean target feature_extractor/encoder/bn1/running_mean feature_extractor/encoder/bn1/running_mean of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
            "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1586: UserWarning: Node feature_extractor_encoder_bn1_running_var target feature_extractor/encoder/bn1/running_var feature_extractor/encoder/bn1/running_var of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
            "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1586: UserWarning: Node feature_extractor_encoder_layer1_0_bn1_running_mean target feature_extractor/encoder/layer1/0/bn1/running_mean feature_extractor/encoder/layer1/0/bn1/running_mean of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
            "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1586: UserWarning: Node feature_extractor_encoder_layer1_0_bn1_running_var target feature_extractor/encoder/layer1/0/bn1/running_var feature_extractor/encoder/layer1/0/bn1/running_var of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
            "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1586: UserWarning: Node feature_extractor_encoder_layer1_0_bn2_running_mean target feature_extractor/encoder/layer1/0/bn2/running_mean feature_extractor/encoder/layer1/0/bn2/running_mean of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
            "  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '\n",
            "/usr/local/lib/python3.10/dist-packages/torch/fx/graph.py:1593: UserWarning: Additional 291 warnings suppressed about get_attr references\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied 325 of general pattern rewrite rules.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_program.save(\"my_image_classifier.onnx\")"
      ],
      "metadata": {
        "id": "8AbNHe2ToYiy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b2seBAmmo_U7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}