{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAd2uZoqr5i290Utrcflh/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yusuku/DepthEstimationFrom360/blob/main/Error1FreDSNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYHAZDqo0TJu",
        "outputId": "35440e4a-937f-453d-b0da-9bf9d5bb775c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.25.5)\n",
            "Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from collections import OrderedDict\n",
        "\n",
        "def count_params(a):\n",
        "    out = sum(p.numel() for p in a.parameters())\n",
        "    return out\n",
        "\n",
        "#Resnet\n",
        "ENCODER_RESNET = [\n",
        "    'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152',\n",
        "    'resnext50_32x4d', 'resnext101_32x8d'\n",
        "]\n",
        "class Resnet(nn.Module):\n",
        "    def __init__(self, backbone='resnet50', pretrained=True):\n",
        "        super(Resnet, self).__init__()\n",
        "        assert backbone in ENCODER_RESNET\n",
        "        self.encoder = getattr(models, backbone)(pretrained=pretrained)\n",
        "        del self.encoder.fc, self.encoder.avgpool\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        x = self.encoder.conv1(x)\n",
        "        x = self.encoder.bn1(x)\n",
        "        x = self.encoder.relu(x)\n",
        "        x = self.encoder.maxpool(x)\n",
        "\n",
        "        x = self.encoder.layer1(x);  features.append(x)  # 1/4\n",
        "        x = self.encoder.layer2(x);  features.append(x)  # 1/8\n",
        "        x = self.encoder.layer3(x);  features.append(x)  # 1/16\n",
        "        x = self.encoder.layer4(x);  features.append(x)  # 1/32\n",
        "        return features\n",
        "\n",
        "    def list_blocks(self):\n",
        "        lst = [m for m in self.encoder.children()]\n",
        "        block0 = lst[:4]\n",
        "        block1 = lst[4:5]\n",
        "        block2 = lst[5:6]\n",
        "        block3 = lst[6:7]\n",
        "        block4 = lst[7:8]\n",
        "        return block0, block1, block2, block3, block4\n",
        "\n",
        "class CircConv(nn.Module):\n",
        "    def __init__(self,in_depth,out_depth, AF='prelu',BN=False,ks=3):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.in_depth = in_depth\n",
        "        self.out_depth = out_depth\n",
        "        self.AF = AF\n",
        "        self.BN = BN\n",
        "        #Layers\n",
        "        self.Conv = nn.Conv2d(self.in_depth,self.out_depth,ks,1,ks//2,padding_mode='circular')\n",
        "        self.prelu = nn.PReLU(self.out_depth)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.bn = nn.BatchNorm2d(self.out_depth)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.Conv(x)\n",
        "        if self.BN:\n",
        "            out = self.bn(out)\n",
        "        if self.AF == 'relu':\n",
        "            out = self.relu(out)\n",
        "        elif self.AF == 'prelu':\n",
        "            out = self.prelu(out)\n",
        "        elif self.AF == 'sigmoid':\n",
        "            out = self.sigmoid(out)\n",
        "        else:\n",
        "            out = out\n",
        "        return out\n",
        "\n",
        "#Initial Convolutional block\n",
        "class WConv(nn.Module):\n",
        "    def __init__(self,in_depth, out_depth, AF=None, BN=False, p=0.0):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.in_depth = in_depth\n",
        "        self.out_depth = out_depth\n",
        "        #Layers\n",
        "        self.layer1 = nn.Sequential(CircConv(self.in_depth,self.in_depth//4     ,AF=AF,BN=BN,ks=1),\n",
        "                                    nn.Dropout2d(p),\n",
        "                                    CircConv(self.in_depth//4,self.in_depth//4  ,AF=AF,BN=BN,ks=3),\n",
        "                                    nn.Dropout2d(p),\n",
        "                                    CircConv(self.in_depth//4,self.out_depth    ,AF=AF,BN=BN,ks=1))\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.layer1(x)\n",
        "        return out\n",
        "\n",
        "class Spectra(nn.Module):\n",
        "    def __init__(self,in_depth,AF='prelu'):\n",
        "        super().__init__()\n",
        "\n",
        "        #Params\n",
        "        self.in_depth = in_depth\n",
        "        self.inter_depth = self.in_depth//2 if in_depth>=2 else self.in_depth\n",
        "\n",
        "        #Layers\n",
        "        self.AF1 = nn.ReLU if AF=='relu' else nn.PReLU(self.inter_depth)\n",
        "        self.AF2 = nn.ReLU if AF=='relu' else nn.PReLU(self.inter_depth)\n",
        "        self.inConv = nn.Sequential(nn.Conv2d(self.in_depth,self.inter_depth,1),\n",
        "                                    nn.BatchNorm2d(self.inter_depth),\n",
        "                                    self.AF1)\n",
        "        self.midConv = nn.Sequential(nn.Conv2d(self.inter_depth,self.inter_depth,1),\n",
        "                                    nn.BatchNorm2d(self.inter_depth),\n",
        "                                    self.AF2)\n",
        "        self.outConv = nn.Conv2d(self.inter_depth, self.in_depth, 1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.inConv(x)\n",
        "        skip = copy.copy(x)\n",
        "        rfft = torch.fft.rfft2(x)\n",
        "        real_rfft = torch.real(rfft)\n",
        "        imag_rfft = torch.imag(rfft)\n",
        "        cat_rfft = torch.cat((real_rfft,imag_rfft),dim=-1)\n",
        "        cat_rfft = self.midConv(cat_rfft)\n",
        "        mid = cat_rfft.shape[-1]//2\n",
        "        real_rfft = cat_rfft[...,:mid]\n",
        "        imag_rfft = cat_rfft[...,mid:]\n",
        "        rfft = torch.complex(real_rfft,imag_rfft)\n",
        "        spect = torch.fft.irfft2(rfft)\n",
        "        out = self.outConv(spect + skip)\n",
        "        return out\n",
        "\n",
        "class FastFC(nn.Module):\n",
        "    def __init__(self,in_depth,AF='prelu'):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.in_depth = in_depth//2\n",
        "\n",
        "        #Layers\n",
        "        self.AF1 = nn.ReLU if AF=='relu' else nn.PReLU(self.in_depth)\n",
        "        self.AF2 = nn.ReLU if AF=='relu' else nn.PReLU(self.in_depth)\n",
        "        self.conv_ll = nn.Conv2d(self.in_depth,self.in_depth,3,padding='same')\n",
        "        self.conv_lg = nn.Conv2d(self.in_depth,self.in_depth,3,padding='same')\n",
        "        self.conv_gl = nn.Conv2d(self.in_depth,self.in_depth,3,padding='same')\n",
        "        self.conv_gg = Spectra(self.in_depth, AF)\n",
        "        self.bnaf1 = nn.Sequential(nn.BatchNorm2d(self.in_depth),self.AF1)\n",
        "        self.bnaf2 = nn.Sequential(nn.BatchNorm2d(self.in_depth),self.AF2)\n",
        "\n",
        "    def forward(self,x):\n",
        "        mid = x.shape[1]//2\n",
        "        x_loc = x[:,:mid,:,:]\n",
        "        x_glo = x[:,mid:,:,:]\n",
        "        x_ll = self.conv_ll(x_loc)\n",
        "        x_lg = self.conv_lg(x_loc)\n",
        "        x_gl = self.conv_gl(x_glo)\n",
        "        x_gg = self.conv_gg(x_glo)\n",
        "        out_loc = torch.add((self.bnaf1(x_ll + x_gl)),x_loc)\n",
        "        out_glo = torch.add((self.bnaf2(x_gg + x_lg)),x_glo)\n",
        "        out = torch.cat((out_loc,out_glo),dim=1)\n",
        "        return out,out_loc,out_glo\n",
        "\n",
        "class FourierBlock(nn.Module):\n",
        "    def __init__(self,num_layer,in_depth,return_all=True):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.num_layers = num_layer\n",
        "        self.in_depth = in_depth\n",
        "        self.return_all = return_all\n",
        "        #Layers\n",
        "        self.block = nn.ModuleList()\n",
        "        for _ in range(self.num_layers):\n",
        "            self.block.append(FastFC(self.in_depth,'prelu'))\n",
        "\n",
        "    def forward(self,x):\n",
        "        for layer in self.block:\n",
        "            x,x_loc,x_glo = layer(x)\n",
        "        if self.return_all:\n",
        "            return x,x_loc,x_glo\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,features_depth,latent_depth):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.features_depth = features_depth\n",
        "        self.num_maps = len(features_depth)\n",
        "        self.latent_depth = latent_depth\n",
        "\n",
        "        #Layers\n",
        "        self.FB1 = FourierBlock(1,self.features_depth[0],False)\n",
        "        self.down1 = nn.Upsample(scale_factor=0.5,mode='bilinear',align_corners=False)\n",
        "        self.convB1 = WConv(self.features_depth[0],self.features_depth[1],AF='prelu',BN=True, p=0.4)\n",
        "        self.FB2 = FourierBlock(1,self.features_depth[1],False)\n",
        "        self.down2 = nn.Upsample(scale_factor=0.5,mode='bilinear',align_corners=False)\n",
        "        self.convB2 = WConv(self.features_depth[1],self.features_depth[2],AF='prelu',BN=True, p=0.4)\n",
        "        self.FB3 = FourierBlock(1,self.features_depth[2],False)\n",
        "        self.down3 = nn.Upsample(scale_factor=0.5,mode='bilinear',align_corners=False)\n",
        "        self.convB3 = WConv(self.features_depth[2],self.features_depth[3],AF='prelu',BN=True, p=0.4)\n",
        "        self.FB4 = FourierBlock(1,self.features_depth[3],False)\n",
        "        self.convB4 = WConv(self.features_depth[3],self.latent_depth     ,AF='prelu',BN=True, p=0.4)\n",
        "\n",
        "    def forward(self,x):\n",
        "        flow = x[0]\n",
        "        f1 = self.FB1(flow)\n",
        "        flow = torch.add(self.convB1(self.down1(f1)),x[1])\n",
        "        f2 = self.FB2(flow)\n",
        "        flow = torch.add(self.convB2(self.down2(f2)),x[2])\n",
        "        f3 = self.FB3(flow)\n",
        "        flow = torch.add(self.convB3(self.down3(f3)),x[3])\n",
        "        f4 = self.FB4(flow)\n",
        "        out = self.convB4(f4)\n",
        "        inter_features = [f4,f3,f2,f1]\n",
        "        return out,inter_features\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,latent_depth,feat_depth):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.in_depth = latent_depth\n",
        "        self.feat_depth = feat_depth\n",
        "\n",
        "        #Layers\n",
        "        self.convB4 = WConv(self.in_depth,self.feat_depth[0],AF='prelu',BN=True)\n",
        "        self.alpha4 = nn.Parameter(torch.randn((1)))\n",
        "        self.FB4 = FourierBlock(1,self.feat_depth[0],False)\n",
        "\n",
        "        self.convB3 = WConv(self.feat_depth[0],self.feat_depth[1],AF='prelu',BN=True)\n",
        "        self.up3 = nn.Upsample(scale_factor=2,mode='bilinear',align_corners=False)\n",
        "        self.alpha3 = nn.Parameter(torch.randn((1)))\n",
        "        self.FB3 = FourierBlock(1,self.feat_depth[1],False)\n",
        "\n",
        "        self.convB2 = WConv(self.feat_depth[1],self.feat_depth[2],AF='prelu',BN=True)\n",
        "        self.up2 = nn.Upsample(scale_factor=2,mode='bilinear',align_corners=False)\n",
        "        self.alpha2 = nn.Parameter(torch.randn((1)))\n",
        "        self.FB2 = FourierBlock(1,self.feat_depth[2],False)\n",
        "\n",
        "        self.convB1 = WConv(self.feat_depth[2],self.feat_depth[3],AF='prelu',BN=True)\n",
        "        self.up1 = nn.Upsample(scale_factor=2,mode='bilinear',align_corners=False)\n",
        "        self.alpha1 = nn.Parameter(torch.randn((1)))\n",
        "        self.FB1 = FourierBlock(1,self.feat_depth[3],False)\n",
        "\n",
        "        self.U1 = nn.Sequential(WConv(self.feat_depth[3],self.feat_depth[4],AF='prelu',BN=True),\n",
        "                                nn.Upsample(scale_factor=2,mode='bilinear',align_corners=False),\n",
        "                                FourierBlock(1,self.feat_depth[4],False))\n",
        "\n",
        "        self.U2 = nn.Sequential(WConv(self.feat_depth[4],self.feat_depth[5],AF='prelu',BN=True),\n",
        "                                nn.Upsample(scale_factor=2,mode='bilinear',align_corners=False),\n",
        "                                FourierBlock(1,feat_depth[5],False))\n",
        "\n",
        "    def forward(self,x,inter_features):\n",
        "        x = torch.add(self.convB4(x),inter_features[0]*self.alpha4[0])\n",
        "        f1 = self.FB4(x)\n",
        "        x = torch.add(self.up3(self.convB3(f1)),inter_features[1]*self.alpha3[0])\n",
        "        f2 = self.FB3(x)\n",
        "        x = torch.add(self.up2(self.convB2(f2)),inter_features[2]*self.alpha2[0])\n",
        "        f3 = self.FB2(x)\n",
        "        x = torch.add(self.up1(self.convB1(f3)),inter_features[3]*self.alpha1[0])\n",
        "        f4 = self.FB1(x)\n",
        "        f5 = self.U1(f4)\n",
        "        f6 = self.U2(f5)\n",
        "        upscale_features = [f1,f2,f3,f4,f5,f6]\n",
        "        return upscale_features\n",
        "\n",
        "class SemanticBranch(nn.Module):\n",
        "    def __init__(self,inter_depth,num_classes,features_depth):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.inter_depth = inter_depth\n",
        "        self.num_classes = num_classes\n",
        "        self.feat_depths = features_depth\n",
        "        self.num_feat_maps = len(self.feat_depths)\n",
        "        #Layers\n",
        "        self.alphas = nn.ParameterList()\n",
        "        self.ScaleMediator = nn.ModuleList()\n",
        "        for i in range(self.num_feat_maps):\n",
        "            self.alphas.append(nn.Parameter(torch.randn(1)))\n",
        "            self.ScaleMediator.append(nn.Sequential(WConv(self.feat_depths[i],self.inter_depth,AF='relu',BN=True),\n",
        "                                                    nn.UpsamplingBilinear2d(scale_factor=(2**(self.num_feat_maps-i-1)))))\n",
        "        self.outSemanticConv = CircConv(self.inter_depth, self.num_classes, AF='relu', BN=True, ks=3)\n",
        "\n",
        "    def forward(self,feat_list):\n",
        "        out = self.ScaleMediator[0](feat_list[0])\n",
        "        for i in range(1,self.num_feat_maps):\n",
        "            out += self.ScaleMediator[i](feat_list[i]) * self.alphas[i]\n",
        "        out = self.outSemanticConv(out)\n",
        "        return out\n",
        "\n",
        "class DepthBranch(nn.Module):\n",
        "    def __init__(self,inter_depth,features_depth):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        self.inter_depth = inter_depth\n",
        "        self.feat_depths = features_depth\n",
        "        self.num_feat_maps = len(self.feat_depths)\n",
        "        #Layers\n",
        "        self.alphas = nn.ParameterList()\n",
        "        self.ScaleMediator = nn.ModuleList()\n",
        "        for i in range(self.num_feat_maps):\n",
        "            self.alphas.append(nn.Parameter(torch.randn(1)))\n",
        "            self.ScaleMediator.append(nn.Sequential(WConv(self.feat_depths[i],self.inter_depth,AF='prelu',BN=True),\n",
        "                                                    nn.UpsamplingBilinear2d(scale_factor=(2**(self.num_feat_maps-i-1)))))\n",
        "\n",
        "        self.outDepthConv = CircConv(self.inter_depth,1,'relu',False)\n",
        "\n",
        "    def forward(self,feat_list):\n",
        "        out = self.ScaleMediator[0](feat_list[0])\n",
        "        for i in range(1,self.num_feat_maps):\n",
        "            out += self.ScaleMediator[i](feat_list[i]) * self.alphas[i]\n",
        "        out = self.outDepthConv(out)\n",
        "        return out\n",
        "\n",
        "class FDS(nn.Module):\n",
        "    ''' Main network body for semantic segmentation and depth estimation\n",
        "        from single panoramas (equirectangular for now) -> Use of EquiConvs?'''\n",
        "    x_mean = torch.FloatTensor(np.array([0.485,0.456,0.406])[None, :, None, None])\n",
        "    x_std  = torch.FloatTensor(np.array([0.229,0.224,0.225])[None, :, None, None])\n",
        "\n",
        "    def __init__(self,num_classes,backbone='resnet50'):\n",
        "        super().__init__()\n",
        "        #Params\n",
        "        ## Non-learnable parameters\n",
        "        self.latent_depth = 1024\n",
        "        self.num_classes = int(num_classes)\n",
        "        ## Architecture parameters\n",
        "        self.backbone = backbone\n",
        "        self.semantic_inter = 128\n",
        "        self.depth_inter = 128\n",
        "        ## Auxiliar data\n",
        "        self.features_depth = [256,512,1024,2048] #Depth from ResNet layers\n",
        "        self.feat_scale = [1/8,1/4,1/2,1]\n",
        "\n",
        "        #Layers\n",
        "        ##Encoder part\n",
        "        self.feature_extractor = Resnet(self.backbone) # resnet18, resnet50 ¿resnet101?\n",
        "        self.Encoder = Encoder(self.features_depth,self.latent_depth)\n",
        "\n",
        "        ##Decoder part\n",
        "        self.decoder_depth = [2048,1024,512,256,64,16]\n",
        "        self.Decoder = Decoder(self.latent_depth,self.decoder_depth)\n",
        "\n",
        "        #Semantic branch\n",
        "        self.depmaps_seg = [1024,512,256,64,16]\n",
        "        self.SemanticSegmentator = SemanticBranch(self.semantic_inter,self.num_classes,self.depmaps_seg)\n",
        "\n",
        "        #Depth branch\n",
        "        self.depmaps_depth = [256,64,16]\n",
        "        self.DepthEstimator = DepthBranch(self.depth_inter,self.depmaps_depth)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self._prepare_x(x)\n",
        "        output = {}\n",
        "\n",
        "        #Feature extraction\n",
        "        feature_list = self.feature_extractor(x)\n",
        "        enc_features,inter_features = self.Encoder(feature_list)\n",
        "\n",
        "        #Decoder\n",
        "        upscale_features = self.Decoder(enc_features,inter_features)\n",
        "\n",
        "        #Depth prediction\n",
        "        output['Depth'] = self.DepthEstimator(upscale_features[-len(self.depmaps_depth):])\n",
        "\n",
        "        #Semantic branch\n",
        "        output['Semantic'] = self.SemanticSegmentator(upscale_features[-len(self.depmaps_seg):])\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _prepare_x(self, x):\n",
        "        if self.x_mean.device != x.device:\n",
        "            self.x_mean = self.x_mean.to(x.device)\n",
        "            self.x_std = self.x_std.to(x.device)\n",
        "        return (x[:, :3] - self.x_mean) / self.x_std\n",
        "\n",
        "    def param_count_sections(self):\n",
        "        bkb_params = count_params(self.feature_extractor)\n",
        "        enc_params = count_params(self.Encoder)\n",
        "        dec_params = count_params(self.Decoder)\n",
        "        seg_params = count_params(self.SemanticSegmentator)\n",
        "        dep_params = count_params(self.DepthEstimator)\n",
        "        all_params = np.sum([bkb_params,enc_params,dec_params,seg_params,dep_params])\n",
        "        print('Feature extractor parameters: %.2f M' %(bkb_params/1e6))\n",
        "        print('Encoder parameters: %.2f M' %(enc_params/1e6))\n",
        "        print('Decoder parameters: %.2f M' %(dec_params/1e6))\n",
        "        print('Segmentation branch parameters: %.2f M' %(seg_params/1e6))\n",
        "        print('Depth branch parameters: %.2f M' %(dep_params/1e6))\n",
        "        print('Total number of parameters: %.2f M' %(all_params/1e6))"
      ],
      "metadata": {
        "id": "sAEyS7HQzzpX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EDEdDvzw7MBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cpu')\n",
        "batch = 1\n",
        "ch,h,w = 3,512,1024\n",
        "torch_input = torch.rand((batch,ch,h,w))\n",
        "torch_input = torch_input.to(device)\n",
        "\n",
        "torch_model=FDS(14)\n",
        "torch_model.to(device)\n",
        "torch_model.eval()\n",
        "torch_output = torch_model(torch_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9t8onKS3GI2",
        "outputId": "ea45de87-f8ff-4fdb-f4f3-16f9250f69a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    }
  ]
}